---
layout: post
comments: true
title:  "Navigating the game of peer reviews"
description: "How to navigate peer review as a phd student?"
date:   2026-02-23 11:00:00
---
> 
Peer review is meant to be double blind. This means you don't know who your reviewers are. Your reviewers dont know who you are. And you can exploit that anonymity to your advantage.
<div style="border: 3px solid #FF0000; padding: 20px; margin: 20px 0; color: #FF0000; font-family: 'Courier New', Courier, monospace;  font-weight: bold; line-height: 1.4;">
  First a disclaimer: All the advice in this article is risky, controversial, and not safe for work. Skip if you are less than 18 years of age. Everyone deserves an equal playing field. This is not a substitute for hardwork/genuine research, which often takes decades. 
</div>

<div style="text-align: center;"> 
    <img 
        class="img-fluid" 
        src="{{ site.baseurl }}/assets/img/peer-review/arjuna.png" 
        style="width: 50%; height: auto; display: block; margin: 0 auto;" 
        >
</div>

In earlier years of my phd, when i was freshly minted out of my college,  i thought that i will change the world (and win a nobel prize). I had a big ego problem, and thought there is no one greater than me. My hands were not folded in humility that time(like the man named arjuna in the left side of the photograph). Thankfully, God (on the right, called krishna) had a different lesson in store for me. 

It `only' took 2 rounds of broken shoulders, two visits to ER, one week of stay in a hospital, academic probations, losing my mom to cancer, a semester off from PhD, family getting scammed of 100k dollars (twice), some weed, to attain this state of nirvana (and some sweet sprinkle of anxiety, depression medicine, and therapy). So, i wish that no one else has to learn the bitter lesson god made me learn the hard way.

Here is how a typical peer-review cycle went : 1) Got a lot of bad reviews 2) Felt upset, and stayed in bed for a whole day binge watching movies. 3) abused the reviewer who gave bad scores, felt he didnt understand anything 4) complained a little bit to my advisor, threatened i will leave my phd (as if i am doing him a favour by doing phd in the first place). 5) withdrew the paper if it is not salvagable, and resubmitted to the next venue. 6) forgot my worries for the next two months, until this cycle repeated, and i come back to 1). I stayed in this loop for around 3 years of my phd, and never made any progress. 

To make matters worse, when other people got into tier 1 conferences, it hurted my ego. I never could understand what made them successful. Chances are, you are also stuck in a similar loop, and don't know what to do. You think that peer-review is a "random" game, where most of the reviewers are incompetent and you just landed a bad set of reviewers. You attribute someone else's success to `luck', or 'they had a good day'. You have successfully managed to fool your brain into such an illusion. Congratulations. 

So, today we will try to  ask: what makes some people good at the game of peer-review? Are there any heuristics, we could learn from them, and apply to our own research? On surface, it seems pretty simple: do good research, write good papers, and you will sail through peer-review. However, the reality is sad: good research often get rejected.

In the beginning i learnt a lot from Devi Parikhs [amazing advice](https://deviparikh.medium.com/how-we-write-rebuttals-dc84742fece1). Additionally, time taught me a few more things which appear to work well on the reviewer 2. The reviewer who will burn your paper to ashes. The reviewer who gives a strong reject, with a confidence of 5. Who is unwilling to budge. He will take down your paper. He will flip the reviewers who supported you. He will drive a nail in your coffin. Your hands will tremble as you withdraw your paper. Or if you are brave, you wait for final results are a reject anyways.  


However, you like me will rise from your ashes. Now this reviewer 2, needs some special  kind of handling. I will try to formalize that mechanism here:


- black ops analogy: imagine yourself as a part of swat team. Your reviewers are your targets. Your job is to hunt them. That requires precision. We will build a fortress first. 

Designing fortress (bulletproofing paper).

- Your paper should inherently use terms like "kindly note, please note...". It intuitively lowers reviewer defence. A little politeness goes long way. 
- Setting baselines: If there is a paper in the area you work, copy their table. Those serve as baselines. Add all baselines uptil last 6 months. Sounds obvious, most don't do it. 
    - arxiv allows an option to download papers source code (in latex). just copy table code from there. Dont start drawing table again. 
- If it is a brand new problem statement, things get messy, take time to figure out realistic baselines. 
    - Maybe ask Gemini (or sota LLM)
- Set expectations: If you are from academia, mention the lack of resources. Do best to get the results. 
- Figures: Most people will tell you to have colorful figures. The most impactful papers have simple black and white figures (check word2vec). No colors. Psychologically, it tells reviewer, " my idea is so strong , i dont need a complex figure to explain it". Humans already live complex lives. Give them simplicity. 
- The illusion of identity.
    - Most people upload their papers on arxiv/twitter. Everyone knows your name, which sometimes backfires.  If you come from a company, or a big hot shot lab, you can play the prestige game. But, those who don't have it, cant play it. Anonymity is your weapon. 
    - The key is to psychologically install a `potential identity' in the reviewers mind. Mirror the style of other labs (that's not forbidden!!). 
    - A very famous example is hinton, notice how all his slides, and figures have a pale yellow background. That's his signature.  
- Stupid cartoons don't survive test of time. So do stupid titles. 
    - People put smileys, robots , cartoons in figures. Dont do that. Be mature. 
    - I put a fucking godzilla in my nips paper. now that i see it, it was a foolish move. 
    - People like the word "machines". They also like looking at things they secretly desire but dont get in real life. Give them machines. Boltzmann machines. Perception Machines. Time Dilation Networks. Neural Nets in Superposition. Artificial Karamuto Oscillatory Neurons. See what i am doing?
    - AI rests on psychology, physics, neuroscience. If you can steal a term from there, even better. 
- Detach your worth: Psychopaths don't get attached to their target. Don't be attached to your paper. Use third tense: "this paper reveals", "this work shows", "Results suggest that this phenomenon is true".
    - The illusion you want to pass is that you did the work, results came, and anyone can reproduce it. You have discovered some universal truth, a law of nature which machines emulate. Your job now is merely to report the results like a newsreporter. DONT GET EMOTIONAL. 
- People use "will", u shall use "shall": Replace 'will' with `shall'. 
    - Why does relegious scripture sound so authoritative? Because jesus "sayeth", man "witnessed". Behold o reader, let reviewer witness your paper. (Sounds cheesy, i know, but it still works) 
- Don't claim sota: Speak less, show more (really important!!). In contributions never say you beat sota. 
    - Beat is too rough. If the person whose method you beat ends up as your reviewer, it will hurt his ego, he'll give reject. 
    - Say "we demonstrate competitive results over prior state of the art".
- Never say we 'solved' a problem, say we `tried' to solve the problem. 
- Write future work in limitations section instead of writing limitations.
    - People will tell you it's wrong. My advisor taught me this counter-intuitive trick. 
    - In rebuttal you can write: "we apologize that we showed optimism/came across as enthusiastic in our limitations". Our limitations are <>.
        - This creates a reverse psychology. Gain one more point in your favour. 
- Create a flop matched baseline: In ablations, include one line that your method's improvements are not due to simply increasing the no of parameters, and you actually checked by training a larger model of the baseline. 

- Overdress up your method by some silly name: Let's say you are just selling a MLP which takes time as input. It is just a frigging MLP. 
    - But no, you `have to' call it "the dense-temporal encoder". Reviewers love fancy names. Give them that. Sells really well. (not that i prefer it, i can see right through bullshit)
    - reviewers at cvpr/eccv tend to be superficial. Quality of reviews at icml/iclr/nips tends to be better. 
        - however, it is changing soon, since vision people (including me) are bombarding ml conferences with their shitty papers, and entire peer-review has gone downhill.

- Imagine you have a method. It has two blocks. 
    - Draw a single teaser with those two blocks 
    - Then explain them one by one. Each section should contain its own figure for that block 
    - Every subsection has 3 parts: Intuition, mathematical equation, and a takeaway.

- Techniques for `quick' tier1-publications: 
    - I don't appreciate them, but i will be honest: i did resort to them in my PhD. I have to promise myself not to do it again, and do actual research. 
    - Work on something with less data, partially labelled data, semi-supervised learning, constrained-settings. 
    - Open vocabulary also works. You can always strap an adapter to a big llm. And train it with a lora. Of course, you are free to quantize the lora-adapter.
    - build your own problem, own benchmark, and own models (nobody will care out eventually). play the "dataset and benchmark" game. 
    - build a problem where every method fails. For eg, learning from one sample. 
    - extend image based models to videos, especially long videos, not many people do videos. 
        - call your paper long term video. claim it works for 20k frames. sells really well. :-)
    - robustness: corrupt inputs to models, check their robustness, techniques for improving them in real world. 
    - Freeze model, finetune some prompts. Call it "adaptation"
    
- Many people told me they still wanna play this game, just `to get some papers'.
    -  Trust me, seeing an influx of tier 1 papers accepted this way, creates a hunger for more, and it is really difficult to stop doing that. 
        
- Most don't release code.
    - Unfortunately, some advisors pressure their students to play this game. My deepest sympathies for you. 
    - Dr. Takeo Kanade came to our lab, and said: "only 3 people read your paper, you, your advisor, and reviewer'. Then, it dies a quick death once published. No-one will care about it. The only thing it will satisfy is your ego. But egos are frail. Your sense of achievement/happiness will last for about a week, and then disappear. 
    - Whatever you built is garbage if no-one uses it. No matter how smart you are. Keep this in mind. 
- while writing ablations question yourself: what all hyperparameters i introduced?. what will happen if i vary one of them whilst holding other constant?. 
    - here is the most important question: of all those variations, which ablation offers `something new' to the reviewer. only show that in the main paper. for eg, nobody wants to see data augmentation improved your network, everyone knows that.
    - often i have made the mistake of doing all possible ablations, and cluttering my paper. 
        - don't be a fool like me. 
    - holding all parameters except one is really important: if you have 10 variabels, you only need to do 10 ablations this way. Otherwise, it quickly becomes 2 to the power 10. I made this blunder, and wasted a year. 
    - sometimes we tend to train our models with all possible variations of hyperparameters to get the best result. 
        - dont make this mistake. 
