---
layout: post
comments: true
title:  "The nature of soul in a learning machine."
description: "We assume that soul is a human's identity, and resides in heart. But, intelligence is constrained to brain and 'soul' can be equated to a machine's learning experience."
date:   2021-02-02 11:00:00
---

> It is said that the soul is indivisible, inconceivable, and immutable. Knowing this, you should not grieve for the body. This individual soul is unbreakable and insoluble and can be neither burned nor dried "

As a person puts on new garments, giving up the old ones, the soul similarly accepts new material bodies, giving up the old and useless ones. Thus, the seers of truth have concluded that of material body, their is no endurance and of the supreme soul there is no change. : Says Lord Krishna to Arjuna, Srimad Bhagavad Gita.

Till now, i have shown that in an emotion machine, individual emotions shall act as agents. Each emotion, will cause the machine to traverse individual goal trees and reflect those changes in the agents physical actions. However, what troubles me the mechanism which occurs at the roots of these goal trees. Consider for eg, that i devised a machine which is capable of feeling emotions. Then, i would have abstract implementations of ideas like "fear", hunger" etc. But, what is the mechanism which helps us to select what we feel? In particular, given many goal trees[emulating emotions] , how do we know which ones should we choose? Is that what we call the self, the soul, the supreme? If yes, can it be built into a machine?

First, let me explain why the nature of soul troubles me. Lord Krishna mentions that a soul is always unchanging & that the soul remains same across material bodies. This looks ok from a computational viewpoint, i.e. when a child is born he knows nothing. So, between consecutive births, it seems that the soul 'forgets' the experiences of its past journey. Thus, in the life span of the machine, the learning process seems to start from the scratch. [child starts learning from scratch]. During the entire span the machine [i,e.child] exists, the soul learns and effects the personality of the machine. However, as soon as the child dies , the soul loses it's identity and learning process starts afresh. Two points trouble me: Firstly, if a machine could emulate soul, and died due to mechanical failures, then could it transfer the essence of its old soul to a newly created machine? In that case, will the learning process of the newly created machine start from the 'initial injected soul'?

To understand this phenomenon deeper, i focus on what does the soul mean? Is it the current state, or all the experiences which led a parent machine to it's current level of intelligence?. All we have right now, is a notion of transfer learning in a neural network. We can learn weights, and transfer them to a clone model. This newly created clone then starts from the concepts which its parent had learned in the past and trains further. However, it has come to my understanding that merely injecting training weights into the clone would not guarantee that its soul emulates the soul of its parent. 

Consider for example, a typical scenario where we have an optimization landscape, Each point in this represents a matrix of possible weights which a network might have at any moment. Assume, that you converged at a final point. (which i call the soul of the machine). To reach this point, you could have taken many paths. But, all you get in the end is the final point [which you inject into the clone]. Thus, while a clone will possess the 'current' learnt ability of its parent, it doesn't possess the path which it's parent took to get there. If all the soul is, the optimization path which a machine takes during its training, then we need to store this path and give it to a child. However, the problem is not so easy. You cannot simply take cascades of network weights over time, call them as 'experience' and feed to a child machine. Nor can the number of such weights you cascade, account for how much experience a machine gains during its learning phase (since you can gain more experience, in just a simple gradient step). [Later I shall show that 'experience' is not merely the ability to perform a well known task, ].

Consider for example, again that a neural network is initialized with a set of pretrained weights. Will this network ever remember how many steps/pain it took for it's parent to get there? No. Thus, the notion of soul cannot come from the pre-initialization. This is the point at which our existing knowledge fails, and we have to revert back to observing the things in the simplest manner by which they exist in the nature, for only when we understand the basics, can we hope to devise the mathematical theories to explain those phenomenons. 

Therefore, i ask again. Suppose i cut your hand. Will that hand remember that it belonged to you. No. This is evident from the fact that we can implant organs between multiple humans, but not their brains/identities. Hence, it appears that the soul does not reside in physical body (i,e, like the heart), but a diffused concept inside our brains. And since a diffused abstract can be modelled by a neural network, a machine must be able to model the soul, if we can understand what exactly does a soul mean. 

### The constant nature of the soul .

The question which i had started with was 'how a machine could choose between different goal trees emulating emotions'. Now, i shall illustrate my understanding of how this selection procedure shall function. Firstly, this selection is governed by a 'self' agent. All the emotions should be directly connected to this single agent. Next, the 'self' agent must contain two types of learning centres. The first learning center, should be responsible for immediately selecting which of the emotion agent must be activated. The second learning centre, must be responsible for learning global representations, i.e, the ideas which we assume to be 'constant' for our lives. This step is important, because unless a machine possesses the ability to refer to non-changing ideals which it can rely upon, it's functioning at any instant would be directly in response to the current stimuli. This would effectively squash it's intelligence to re-direct its actions towards a long term goal in favour of short term impulses.Hence there would be no notion of future 'larger awards' than the 'meager piecemeals' it recieves by optimizing current objective. Modelling this decision making ability is what you would call self-control.

So, to explain it's existence, consider that you are fed up with someone. At your root, their are two agents 'angry' and 'murder'. The self agent activates 'angry' agent. The 'angry' agent has access to a 'murder' agent. If, angry agent had a direct access to 'murder' agent, you would murder someone everytime you get angry. However, this does not occur in reality. 

Although 'angry' might have access to the 'murder', the 'self' agent intervenes. Its immediate decision has to be to murder the person. However, the second learning centre of the self-agent, i.e. the region responsible for long term goals informs the self agent that it violates long term principles. Therefore, the self essentially has the ability to pre-empt the selection of agents among themselves. 

Thus, all i have explained is that the emotion agents at the root level, must have a network of interconnections among themselves. This essentially, helps the machine establish hierachy of which agent should activate whom. Sitting on the top of all this phenomenon, there is a self agent which essentially has the ability to pre-empt  the machine, in case the agent it selects violates the long term constraints of the self. 

However, a lot of our thinking process occurs without our self 'manually' activating each agent at the lower level. For eg, we feel hungry, sleepy. These are natural emotions which we term as 'primal' instincts. Thus, it seems that the primal emotions dont jump to the self-layer as long as they dont violate long term principles. Thus, all that remains to understand is the nature of interconnections between multiple emotion agents.

Next, i shall show that these interconnections can be both of 'physical' nature or imposed manually by the 'self agent'. Consider the existence of three agents: work, sleep and anger. 

*** Suppose you are working, and start to feel sleepy. Then you 'force' yourself, to think about a person whom you are angry on. This anger kills your sleep and enables you to work further. I want you to think on this phenomenon.  ***

Consider the first level, i.e. you were working and feeling sleepy. How come two agents were able to exist simulateously. The answer is that sleep is one of the primal needs we have. Hence, the agents which are primal in nature might exist at the same time. However, as humans we can only perform function of one 'self-imposed agent' , i.e. work. So, work agent was brought into existence by the self, and anger agent represented the natural primal instinct in the machine. 

My next question is, does the 'work' agent have a direct connection to the 'sleep' agent? No, because if we had wanted to work, we could always switch the sleep off. Had there been a natural connection between these two agents, our bodies would have always worked and never fallen asleep!!!! (bcoz we always want to work to maximize our productivities). Next, i demonstrated that work agent, in fact invoked the 'anger' agent to counter sleep. Does, 'anger' agent have a direct connection to the sleep?  No, for you dont fall asleep whenever you are angry. 

Thus, we conclude that work had no direct connection with either anger or sleep. Also, anger didnt have a direct connection to work or sleep. However, while we were working, our mind told us to conjure an anger agent, which in turn then impacted sleep. Thus, this 'pseudo-connection' was established because of the property of the higher self agent, and NOT a natural connection between agents at the lower level [than the self]. Thus, i essentially describe the soul in two terms: the property to learn the existence of pseudo-links between non-default agents, and the ability to 'retain/learn' certain long term goals. The other question that remains for now, is how will these pseudo-connections be learnt in the self agent. I hope that this shall become more clear with time. 

Finally, i will like to talk about the 'default' connections which exist in the agents which are not a function of our fantasies. For eg, if you are hungry, you eat. Thus 'hunger' and 'eat' will have a natural connection between them. As long as machine remains along this natural level, the self-agent shall not become involved. The ability to stay in this for natural region of connections, without the selfs intervention, is what we describe commonly as 'being in the flow'. The problem starts occuring when there exists multiple agents who dont have natural connections between them. At that time, the machine's flow switches from these agents level to the self, where the establishment of pseudo-connections occurs. Since this is essentially a pre-emptive process, this would require the machine to cease its physical activity altogether. Once the pseudo-links are established, the machine proceeds to the  natural execution. However, retaining the existence of these pseudo-connections requires the self to be activated, which is NOT a natural agent. At this time, the 'long term agent' of the self, starts a timer as to how long the machine should remain in the self-imposed nature of execution. Finally, the self gets tired, these pseudo-connections disappear and the machine falls back to the plain where all it traverses is the natural physical connections between the emotion agents, which the nature has blessed us with for millenia. 

Thus  we are only the beings with natural instincts, with the computational procedures which can model self, and allow interruption in  a natural sequence of goal-execution. However, what still troubles me is that i can model self, and these emotion agents. Even if two machines have similar self, emotion agents and long term goals, yet there is a notion of 'I' between them. Two things however similar, tend to retain their individualities. Can we hope to construct similar functions, but different identities in machines? I shall answer the nature of individuality of next time. 

till then, 
hare krishna .
credits: many thanks to patrick winston and marvin minsky for their valuable thoughts. 
