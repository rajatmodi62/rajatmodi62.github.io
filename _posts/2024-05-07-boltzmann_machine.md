---
layout: post
comments: true
title:  "On the revival of boltzmann machines."
description: "We recently had the privilege of making GLOM work. I cannot reveal much until manuscript is finished under review. This post is on the revival of boltzmann machines."
date:   2024-05-07 11:00:00
---
> One who is not envious but is a kind friend to all living entities, who does not think himself a proprietor and is free from false ego, who is equal in both happiness and distress, who is tolerant, always satisfied, self-controlled, and engaged in devotional service with determination, his mind and intelligence fixed on Me â€“ such a devotee of Mine is very dear to Me- Srimad Bhagavad Gita 2.13-14.

This world is cruel. It forgets you and moves on. No matter what you give it, doesn't hold any significance in the grand scheme of things. The legacy of my advisors K.R. Parthasarathy and C.R. Rao continues. Harish Parthsarathy still lives on. Both passed away last year in 2023. They were some of the most humble men i knew, and hopefully their work in quantum entanglement shall bear some fruit. 

The last decade has shown a lot of progress in neural nets. But all we have done is scale up. AlexNet had one simple idea: stack layers on top of each other, learn internal representation through backprop and the net starts to show some interesting emergent properties. You could make it see an image, sing a sonnet, create a poem, generate a movie and all sort of other nasty things. In turing's paper: "Can machines think", he posited that all the machines ought to show such abilities when they get fast enough. And they do. So it turns out that he is correct on that. 

We have blundered on two of the points turing said. Firstly, he said perhaps their exist computational principles through which machines shall emulate the brain of a man provided they get fast enough. His calculation on the manchester machine seems to show that the machine is already fast and already possesses a large store of paper. The machines of the current time are even faster and possess bigger harddrives. I dont think the answer to machine intelligence lies in scaling up anymore. But all i have seen is companies making bigger  and costlier gpus. This makes no sense. The very intelligence we had started on this path to emulate and help up seems to have gone further from our grasp: research and usage of ai is restricted to a select group of labs and companies due to the monopoly of gpus. 

I propose to ask: "Does intelligence really need that many neurons"